{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6730fecf",
   "metadata": {},
   "source": [
    "## Assignment 3\n",
    "### \\<Sinuo Wang> \\<a1713814>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df2aac6",
   "metadata": {},
   "source": [
    "### 1. Reading dataset and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e967e07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os.path\n",
    "import spacy\n",
    "import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The original dataset on Kaggle huge, which contains 1056660 samples. To make this runable on my local pc, I randomly sampled 10,000 samples from the original dataset on Kaggle Notebook online, so I don't need to download the whole 20GB.\n",
    " Since I sampled these online, I am going to describe how I sampled the data and what attributes I chose. Here are the detailed steps of how I sampled 10,000 data in Kaggle Notebook:\n",
    "1. use random.sample() to select 25,000 article json path\n",
    "2. read the sampled json get the whole body text, and first 5 bibliographies\n",
    "3. match the paper_id to metadata.csv to get more useful attributes of the articles, including title, authors, and journal\n",
    "4. Data cleaning, including remove samples with missing attributes, non-English, and duplicates.\n",
    "5. Then randomly sampled 10,000 samples from the cleaned data\n",
    "5. save to a csv file called 'sampled_df.csv' and download this sampled dataset to my local PC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of na: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "     paper_id                                              title  \\\n0  PMC8419652  Curriculum tinkering in situations of crises a...   \n1  PMC8527883  Similarities and Differences in the Acute-Phas...   \n2  PMC7107128  Effect of insoluble fiber supplementation appl...   \n3  PMC7972757  Narrating Arabic Translation Online: Another P...   \n4  PMC8834262  Roles of Podoplanin in Malignant Progression o...   \n\n                                             authors  \\\n0                        Amin, Nyna; Mahabeer, Pryah   \n1  Coleman, Celeste; Doyle-Meyers, Lara A.; Russe...   \n2      Yokhana, J. S.; Parkinson, G.; Frankel, T. L.   \n3                               Alonayq, Abdulmohsen   \n4  Suzuki, Hiroyuki; Kaneko, Mika K.; Kato, Yukinari   \n\n                         journal  \\\n0              Prospects (Paris)   \n1                  Front Immunol   \n2                      Poult Sci   \n3  When Translation Goes Digital   \n4                          Cells   \n\n                                            abstract  \\\n0  This article interrogates a curriculum recover...   \n1  Understanding SARS-CoV-2 immune pathology is c...   \n2  Two experiments were conducted to study effect...   \n3  This chapter takes a socionarrative approach t...   \n4  Podoplanin (PDPN) is a cell-surface mucin-like...   \n\n                                           body_text  \\\n0  In this section, we provide an overview of Sou...   \n1  The rapid emergence and dissemination of sever...   \n2  Growth and development in layer pullets during...   \n3  The translation sector has seen an increase in...   \n4  Podoplanin (PDPN)/T1α/E11 antigen/PA2.26 antig...   \n\n                                                 bib  \n0  ['Big policies/small world: An introduction to...  \n1  ['Clinical Characteristics of Coronavirus Dise...  \n2  ['The estimation of pepsin, trypsin, papain, a...  \n3  ['Arabic tradition', 'The narrative constructi...  \n4  ['Podoplanin: An emerging cancer biomarker and...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>paper_id</th>\n      <th>title</th>\n      <th>authors</th>\n      <th>journal</th>\n      <th>abstract</th>\n      <th>body_text</th>\n      <th>bib</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>PMC8419652</td>\n      <td>Curriculum tinkering in situations of crises a...</td>\n      <td>Amin, Nyna; Mahabeer, Pryah</td>\n      <td>Prospects (Paris)</td>\n      <td>This article interrogates a curriculum recover...</td>\n      <td>In this section, we provide an overview of Sou...</td>\n      <td>['Big policies/small world: An introduction to...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>PMC8527883</td>\n      <td>Similarities and Differences in the Acute-Phas...</td>\n      <td>Coleman, Celeste; Doyle-Meyers, Lara A.; Russe...</td>\n      <td>Front Immunol</td>\n      <td>Understanding SARS-CoV-2 immune pathology is c...</td>\n      <td>The rapid emergence and dissemination of sever...</td>\n      <td>['Clinical Characteristics of Coronavirus Dise...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PMC7107128</td>\n      <td>Effect of insoluble fiber supplementation appl...</td>\n      <td>Yokhana, J. S.; Parkinson, G.; Frankel, T. L.</td>\n      <td>Poult Sci</td>\n      <td>Two experiments were conducted to study effect...</td>\n      <td>Growth and development in layer pullets during...</td>\n      <td>['The estimation of pepsin, trypsin, papain, a...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>PMC7972757</td>\n      <td>Narrating Arabic Translation Online: Another P...</td>\n      <td>Alonayq, Abdulmohsen</td>\n      <td>When Translation Goes Digital</td>\n      <td>This chapter takes a socionarrative approach t...</td>\n      <td>The translation sector has seen an increase in...</td>\n      <td>['Arabic tradition', 'The narrative constructi...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>PMC8834262</td>\n      <td>Roles of Podoplanin in Malignant Progression o...</td>\n      <td>Suzuki, Hiroyuki; Kaneko, Mika K.; Kato, Yukinari</td>\n      <td>Cells</td>\n      <td>Podoplanin (PDPN) is a cell-surface mucin-like...</td>\n      <td>Podoplanin (PDPN)/T1α/E11 antigen/PA2.26 antig...</td>\n      <td>['Podoplanin: An emerging cancer biomarker and...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = './sampled_df.csv'\n",
    "sampled_df = pd.read_csv(data_path, index_col=0) # index_col = 0 -> avoid fist 'unnamed col'\n",
    "print(f'number of na: {sampled_df.isnull().any().sum()}')\n",
    "sampled_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "   paper_id                                              title  \\\n0         0  Curriculum tinkering in situations of crises a...   \n1         1  Similarities and Differences in the Acute-Phas...   \n2         2  Effect of insoluble fiber supplementation appl...   \n3         3  Narrating Arabic Translation Online: Another P...   \n4         4  Roles of Podoplanin in Malignant Progression o...   \n\n                                             authors  \\\n0                      [Amin, Nyna, Mahabeer, Pryah]   \n1  [Coleman, Celeste, Doyle-Meyers, Lara A., Russ...   \n2    [Yokhana, J. S., Parkinson, G., Frankel, T. L.]   \n3                             [Alonayq, Abdulmohsen]   \n4  [Suzuki, Hiroyuki, Kaneko, Mika K., Kato, Yuki...   \n\n                         journal  \\\n0              Prospects (Paris)   \n1                  Front Immunol   \n2                      Poult Sci   \n3  When Translation Goes Digital   \n4                          Cells   \n\n                                            abstract  \\\n0  This article interrogates curriculum recovery ...   \n1  Understanding SARS-CoV-2 immune pathology crit...   \n2  Two experiments conducted study effects dietar...   \n3  This chapter takes socionarrative approach exa...   \n4  Podoplanin PDPN cell-surface mucin-like glycop...   \n\n                                           body_text  \\\n0  In section provide overview South Africa demog...   \n1  The rapid emergence dissemination severe acute...   \n2  Growth development layer pullets rearing early...   \n3  The translation sector seen increase volunteer...   \n4  Podoplanin PDPN T1 E11 antigen PA2 26 antigen ...   \n\n                                                 bib paper_identifier  \\\n0  [Big policies/small world: An introduction to ...       PMC8419652   \n1  [Clinical Characteristics of Coronavirus Disea...       PMC8527883   \n2  [The estimation of pepsin, trypsin, papain, an...       PMC7107128   \n3  [Arabic tradition, The narrative construction ...       PMC7972757   \n4  [Podoplanin: An emerging cancer biomarker and ...       PMC8834262   \n\n                                  original_body_text  \\\n0  In this section, we provide an overview of Sou...   \n1  The rapid emergence and dissemination of sever...   \n2  Growth and development in layer pullets during...   \n3  The translation sector has seen an increase in...   \n4  Podoplanin (PDPN)/T1α/E11 antigen/PA2.26 antig...   \n\n                                   original_abstract  \n0  This article interrogates a curriculum recover...  \n1  Understanding SARS-CoV-2 immune pathology is c...  \n2  Two experiments were conducted to study effect...  \n3  This chapter takes a socionarrative approach t...  \n4  Podoplanin (PDPN) is a cell-surface mucin-like...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>paper_id</th>\n      <th>title</th>\n      <th>authors</th>\n      <th>journal</th>\n      <th>abstract</th>\n      <th>body_text</th>\n      <th>bib</th>\n      <th>paper_identifier</th>\n      <th>original_body_text</th>\n      <th>original_abstract</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Curriculum tinkering in situations of crises a...</td>\n      <td>[Amin, Nyna, Mahabeer, Pryah]</td>\n      <td>Prospects (Paris)</td>\n      <td>This article interrogates curriculum recovery ...</td>\n      <td>In section provide overview South Africa demog...</td>\n      <td>[Big policies/small world: An introduction to ...</td>\n      <td>PMC8419652</td>\n      <td>In this section, we provide an overview of Sou...</td>\n      <td>This article interrogates a curriculum recover...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Similarities and Differences in the Acute-Phas...</td>\n      <td>[Coleman, Celeste, Doyle-Meyers, Lara A., Russ...</td>\n      <td>Front Immunol</td>\n      <td>Understanding SARS-CoV-2 immune pathology crit...</td>\n      <td>The rapid emergence dissemination severe acute...</td>\n      <td>[Clinical Characteristics of Coronavirus Disea...</td>\n      <td>PMC8527883</td>\n      <td>The rapid emergence and dissemination of sever...</td>\n      <td>Understanding SARS-CoV-2 immune pathology is c...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Effect of insoluble fiber supplementation appl...</td>\n      <td>[Yokhana, J. S., Parkinson, G., Frankel, T. L.]</td>\n      <td>Poult Sci</td>\n      <td>Two experiments conducted study effects dietar...</td>\n      <td>Growth development layer pullets rearing early...</td>\n      <td>[The estimation of pepsin, trypsin, papain, an...</td>\n      <td>PMC7107128</td>\n      <td>Growth and development in layer pullets during...</td>\n      <td>Two experiments were conducted to study effect...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Narrating Arabic Translation Online: Another P...</td>\n      <td>[Alonayq, Abdulmohsen]</td>\n      <td>When Translation Goes Digital</td>\n      <td>This chapter takes socionarrative approach exa...</td>\n      <td>The translation sector seen increase volunteer...</td>\n      <td>[Arabic tradition, The narrative construction ...</td>\n      <td>PMC7972757</td>\n      <td>The translation sector has seen an increase in...</td>\n      <td>This chapter takes a socionarrative approach t...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Roles of Podoplanin in Malignant Progression o...</td>\n      <td>[Suzuki, Hiroyuki, Kaneko, Mika K., Kato, Yuki...</td>\n      <td>Cells</td>\n      <td>Podoplanin PDPN cell-surface mucin-like glycop...</td>\n      <td>Podoplanin PDPN T1 E11 antigen PA2 26 antigen ...</td>\n      <td>[Podoplanin: An emerging cancer biomarker and ...</td>\n      <td>PMC8834262</td>\n      <td>Podoplanin (PDPN)/T1α/E11 antigen/PA2.26 antig...</td>\n      <td>Podoplanin (PDPN) is a cell-surface mucin-like...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "from ast import literal_eval\n",
    "\n",
    "# only need to download these 3 once\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "def pre_processing(df):\n",
    "    path = './processed_df.csv'\n",
    "    if not os.path.isfile(path):\n",
    "        df_c = df.copy()\n",
    "        #no punctuation or non-alphanumeric chars except periods\n",
    "        df_c['body_text'] = df_c.body_text.str.replace('[^A-Za-z0-9\\s+\\-]',' ') #replace special character with a space\n",
    "        df_c['abstract'] = df_c.abstract.str.replace('[^A-Za-z0-9\\s+\\-]',' ') #replace special character with a space\n",
    "\n",
    "        # remove in-text reference e.g., [3-4]\n",
    "        df_c['body_text'] = df_c.body_text.str.replace('\\[.*?\\]',' ')\n",
    "\n",
    "        # remove new lines and back slash\n",
    "        df_c['body_text'] = df_c.body_text.str.replace('\\n',' ')\n",
    "        df_c['body_text'] = df_c.body_text.str.replace('\\s+',' ')\n",
    "\n",
    "        df_c['abstract'] = df_c.abstract.str.replace('\\n',' ')\n",
    "        df_c['abstract'] = df_c.abstract.str.replace('\\s+',' ')\n",
    "\n",
    "        # no stopwords\n",
    "        stops = stopwords.words('english') # english stopwords, because our data is in english\n",
    "        df_c['body_text'] = df_c.body_text.apply(lambda x: \" \".join([word for word in x.split() if word not in stops]))\n",
    "        df_c['abstract'] = df_c.abstract.apply(lambda x: \" \".join([word for word in x.split() if word not in stops]))\n",
    "\n",
    "        df_c['authors'] = df_c.authors.apply(lambda x: x.split(';'))\n",
    "        df_c['authors'] = df_c.authors.apply(lambda x: [name.strip() for name in x if name.strip() != ''])\n",
    "\n",
    "        df_c['paper_identifier'] = df_c['paper_id']\n",
    "        df_c['paper_id'] = [i for i in range(df_c.shape[0])]\n",
    "\n",
    "        # keep the original body text and abstract for showing IR result\n",
    "        df_c['original_body_text'] = df['body_text'].copy()\n",
    "        df_c['original_abstract'] = df['abstract'].copy()\n",
    "\n",
    "\n",
    "        # save to csv file (pre-process is time-consuming, avoid processing everytime)\n",
    "        df_c.to_csv(path)\n",
    "    else:\n",
    "        df_c = pd.read_csv(path,index_col=0,converters={\"bib\": literal_eval,\"authors\": literal_eval}) # body-text col is list of sentences\n",
    "\n",
    "    return df_c\n",
    "\n",
    "processed_df = pre_processing(sampled_df)\n",
    "\n",
    "processed_df.head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the pre-processing step, I processed on body text, abstract, and authors. The detailed steps are as follows:\n",
    "1. Removed non-alphanumeric chars (except '-'): Because they are most likely to be greek letters that used in the equations or experiment variables, which is not informative in out task. However, '-' used a lot in medical named entities, such as 'COVID-19', 'SARS-CoV-2', 'whole-blood', therefore, I did not remove '-'. Otherwise it would have negative impact on the performance of NER and thus the following downstream tasks.\n",
    "\n",
    "2. Removed in-text references, new lines, back slashes and stopwords, because they are not informative, and removing them will not affect the NER and other following downstream tasks.\n",
    "\n",
    "3. Processed authors from a string containing all names seperated by ';\" to a list of individual's name (using .split(';'))\n",
    "\n",
    "4. Renamed the 'paper_id' to 'paper_identifier', and use the index of the sample to be the 'paper_id'\n",
    "\n",
    "Some Notes:\n",
    "\n",
    "- I did not perform lowercasing here, because in biomedical cases there are a lot of abbreviations, which will be specifically resolved in the entity linker pipeline when building the KB.\n",
    "- I also didn't perform lemmatization nor stemming, because they might modify the named entity, and would be difficult to be recognized by the NER model. Also, lemmatization and stemming works on token level, but there are named entities consist of multiple word tokens, and modified a token in a named entity phrase would affect the NER and the following downstream tasks.For example, 'lung kidney lymphatic vascular systems', 'lymphatic' will be stemmed to 'lymphat'.\n",
    "- I did not convert the text to BOW as the assignment spec 1.c said, because I will use scispacy to perform NER, which takes text as input. And also in the 'assignment tips' and assignment specification step 4 (d), we are encouraged to use word embeddings, so I am not using BOW to vectorise the text in this assignment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "31d75afc",
   "metadata": {},
   "source": [
    "### 2. Named Entity Recognition and Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2\n",
      "For maximum performance, you can install NMSLIB from sources \n",
      "pip install --no-binary :all: nmslib\n"
     ]
    }
   ],
   "source": [
    "# ! pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_md-0.5.1.tar.gz\n",
    "# ! pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bc5cdr_md-0.5.1.tar.gz\n",
    "\n",
    "from scispacy.linking import EntityLinker\n",
    "class Knowledge_Base:\n",
    "    def __init__(self, processed_df):\n",
    "        self.processed_df = processed_df\n",
    "\n",
    "        # init nlp model, model will be loaded when bulding kb\n",
    "        self.nlp = None\n",
    "\n",
    "        # init kb, or load kb if it's already built\n",
    "        self.kb = None\n",
    "        self.kb_path = 'knowledge_base.pkl'\n",
    "        if os.path.isfile(self.kb_path):\n",
    "            with open(self.kb_path, 'rb') as kb_pickled:\n",
    "                self.kb = pickle.load(kb_pickled)\n",
    "        else:\n",
    "            self.build_KB_with_entities()\n",
    "\n",
    "    def __init_nlp(self):\n",
    "        self.nlp = spacy.load(\"en_core_sci_sm\") # for NER #en_ner_bc5cdr_md #en_core_sci_sm\n",
    "\n",
    "        # k=1: return the top candidate (concept with the highest score)\n",
    "        # default score thresh = 0.7, but change to 0.8\n",
    "        # umls: Unified Medical Language System.\n",
    "        self.nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\", 'k':1, 'threshold':0.8}) # for getting canonical name and aliases\n",
    "        return\n",
    "\n",
    "    def __init_KB(self):\n",
    "        # This function only pre-process the paper and author part.\n",
    "        kb = {'paper':{id_str:{} for id_str in self.processed_df['paper_id'].tolist()}}\n",
    "        authors_dict = {}\n",
    "        for id,dic in tqdm.tqdm(kb['paper'].items(), desc = 'Initialize KB', unit = ' article'):\n",
    "            title = self.processed_df.loc[self.processed_df['paper_id']==id]['title'].values[0]\n",
    "            dic.update({'title':title})\n",
    "\n",
    "            authors_lst = self.processed_df.loc[self.processed_df['paper_id']==id]['authors'].values[0]\n",
    "            dic.update({'authors':authors_lst})\n",
    "\n",
    "            for author in authors_lst:\n",
    "                if author not in authors_dict.keys():\n",
    "                    authors_dict.update({author:{'paper_lst':{id}}})\n",
    "                else:\n",
    "                    authors_dict[author]['paper_lst'].add(id)\n",
    "\n",
    "        kb.update({'authors':authors_dict})\n",
    "\n",
    "        # init entity part for futher entity related processes\n",
    "        kb.update({'entities':{}})\n",
    "        return kb\n",
    "\n",
    "    def __update_entity_dict(self,entity_dict,doc,linker,paper_id):\n",
    "        for entity in doc.ents: # for each recognised entities\n",
    "            for umls_ent in entity._.kb_ents: # for each matched umls concept matched\n",
    "                # 'umls_ent[0]' is the concept entity with the highest score\n",
    "                concept_id = (linker.kb.cui_to_entity[umls_ent[0]]).concept_id\n",
    "                canonical_name = (linker.kb.cui_to_entity[umls_ent[0]]).canonical_name\n",
    "                definition = (linker.kb.cui_to_entity[umls_ent[0]]).definition\n",
    "                if len((linker.kb.cui_to_entity[umls_ent[0]]).aliases) == 0:\n",
    "                    umls_aliases_set = set()\n",
    "                else:\n",
    "                    umls_aliases_set = set((linker.kb.cui_to_entity[umls_ent[0]]).aliases)\n",
    "\n",
    "                entity_name = (entity.text).strip()\n",
    "\n",
    "                # if this a new entity for both current KB and this paper\n",
    "                if canonical_name not in self.kb['entities'].keys() and canonical_name not in entity_dict.keys():\n",
    "                    if entity.text != canonical_name:\n",
    "                        umls_aliases_set.add(entity_name)\n",
    "\n",
    "                    # entity_dict.update({canonical_name:{'cid': concept_id, 'definition':definition, 'aliases':umls_aliases_set, 'paper_lst':{paper_id}}})\n",
    "                    entity_dict.update({canonical_name:{'cid': concept_id, 'definition':definition, 'aliases':umls_aliases_set, 'paper_lst':{paper_id},'tf_lst':[[paper_id,1]]}})#*\n",
    "\n",
    "                # if it both in kb and the paper, just +1 in term freq\n",
    "                elif canonical_name in self.kb['entities'].keys() and canonical_name in entity_dict.keys():\n",
    "                    for i,tf_lst in enumerate(entity_dict[canonical_name]['tf_lst']):\n",
    "                        if tf_lst[0] == paper_id:\n",
    "                            entity_dict[canonical_name]['tf_lst'][i][1] = entity_dict[canonical_name]['tf_lst'][i][1]+1\n",
    "                            break\n",
    "\n",
    "                # if this entity not in the KB but in this paper\n",
    "                elif canonical_name not in self.kb['entities'].keys() and canonical_name in entity_dict.keys():\n",
    "                    entity_dict[canonical_name]['paper_lst'].add(paper_id)\n",
    "                    for i,tf_lst in enumerate(entity_dict[canonical_name]['tf_lst']):\n",
    "                        if tf_lst[0] == paper_id:\n",
    "                            entity_dict[canonical_name]['tf_lst'][i][1] = entity_dict[canonical_name]['tf_lst'][i][1]+1\n",
    "                            break\n",
    "                    # entity_dict[canonical_name]['tf_lst'][paper_id] = entity_dict[canonical_name]['tf_lst'][paper_id] +1 #*\n",
    "                    if entity_name != canonical_name:\n",
    "                        entity_dict[canonical_name]['aliases'].add(entity_name)\n",
    "\n",
    "\n",
    "                # if this entity appears in th KB, but new to this paper\n",
    "                elif canonical_name in self.kb['entities'].keys() and canonical_name not in entity_dict.keys():\n",
    "                    self.kb['entities'][canonical_name]['paper_lst'].add(paper_id)\n",
    "                    self.kb['entities'][canonical_name]['tf_lst'].append([paper_id,1])\n",
    "\n",
    "                    # self.kb['entities'][canonical_name]['tf_lst'][paper_id] = self.kb['entities'][canonical_name]['tf_lst'][paper_id] + 1\n",
    "                    if entity_name != canonical_name:\n",
    "                        self.kb['entities'][canonical_name]['aliases'].add(entity_name)\n",
    "                    entity_dict.update(self.kb['entities'][canonical_name])\n",
    "\n",
    "        return entity_dict\n",
    "\n",
    "    def __build_all_entity_dict(self,text,title,paper_id,body_text):\n",
    "        # This function is keeping updating this paper's entity dictionary\n",
    "        # during process paper's title, abstract and the first 500 and last 500 words in body text\n",
    "        entity_dict = {}\n",
    "        linker = self.nlp.get_pipe(\"scispacy_linker\")\n",
    "        text_doc = self.nlp(text)\n",
    "        entity_dict.update(self.__update_entity_dict(entity_dict,text_doc,linker,paper_id))\n",
    "\n",
    "        title_doc = self.nlp(title)\n",
    "        entity_dict.update(self.__update_entity_dict(entity_dict,title_doc,linker,paper_id))\n",
    "\n",
    "        body_text_doc = self.nlp(body_text)\n",
    "        entity_dict.update(self.__update_entity_dict(entity_dict,body_text_doc,linker,paper_id))\n",
    "\n",
    "        return entity_dict\n",
    "\n",
    "    def build_KB_with_entities(self):\n",
    "        self.kb = self.__init_KB()\n",
    "        self.__init_nlp()\n",
    "\n",
    "        for paper_id,paper_dict in tqdm.tqdm(self.kb['paper'].items(), desc = 'Building entities', unit = ' article'):\n",
    "            abstract = self.processed_df.loc[self.processed_df['paper_id']==paper_id]['abstract'].values[0]\n",
    "            title = paper_dict['title']\n",
    "            # only process the 1st 500 and the last 500 words in the body text,\n",
    "            # these might be the introduction and the conclusion of the article, which is more representative of this paper\n",
    "            body_text = self.processed_df.loc[self.processed_df['paper_id']==paper_id]['body_text'].values[0]\n",
    "            body_text_intro = ' '.join(body_text.split()[:500])\n",
    "            body_text_conclu = ' '.join(body_text.split()[-500:])\n",
    "            sampled_body_text = body_text_intro + body_text_conclu\n",
    "\n",
    "            entity_dict = self.__build_all_entity_dict(abstract, title,paper_id, sampled_body_text)\n",
    "            for k in ['cid', 'definition', 'aliases', 'paper_lst']:\n",
    "                entity_dict.pop(k, None)\n",
    "            self.kb['entities'].update(entity_dict)\n",
    "            paper_dict.update({'entities': list(entity_dict.keys())})\n",
    "        for canonical in self.kb['entities'].keys():\n",
    "            temp = []\n",
    "            if isinstance(self.kb['entities'][canonical],dict):\n",
    "                for tf_lst in self.kb['entities'][canonical]['tf_lst']:\n",
    "                    if len(temp) == 0:\n",
    "                        temp.append(tf_lst)\n",
    "                    else:\n",
    "                        if tf_lst[0] == temp[-1][0]:\n",
    "                            temp[-1][1] = temp[-1][1]+1\n",
    "                        else:\n",
    "                            temp.append(tf_lst)\n",
    "                self.kb['entities'][canonical].update({'tf_lst':temp})\n",
    "        self.kb['entities'].pop('tf_lst')\n",
    "        self.__save_kb()\n",
    "        return\n",
    "\n",
    "    def __save_kb(self):\n",
    "        with open(self.kb_path,'wb') as kb_pickled:\n",
    "            pickle.dump(self.kb,kb_pickled)\n",
    "            print('Knowledge base saved successfully!')\n",
    "        return\n",
    "\n",
    "\n",
    "k = Knowledge_Base(processed_df)\n",
    "KB = k.kb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The structure of my knowledge base:\n",
    "\n",
    "```\n",
    "├── KB\n",
    "    ├── 'paper'\n",
    "        ├── 'paper_id 1'\n",
    "            ├── 'title' : xxx\n",
    "            ├── 'authors' : [xxx,xxx,...]\n",
    "            ├── 'entities' : [xxx,xxx,...]\n",
    "        ├── 'paper_id 2'\n",
    "            ├── 'title' : xxx\n",
    "            ├── 'authors' : [xxx,xxx,...]\n",
    "            ├── 'entities' : [xxx,xxx,...]\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "\n",
    "    ├── 'authors'\n",
    "        ├── 'author name 1'\n",
    "            ├── 'paper_lst' : [xxx,xxx,...]\n",
    "        ├── 'author name 2'\n",
    "            ├── 'paper_lst' : [xxx,xxx,...]\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "\n",
    "    ├── 'entities'\n",
    "        ├── 'canonical_name 1'\n",
    "            ├── 'cid' (concept id, or CUI) : xxx\n",
    "            ├── 'definition' : xxx\n",
    "            ├── 'aliases' : [xxx,xxx,...]\n",
    "            ├── 'paper_lst' : [xxx,xxx,...]\n",
    "            ├── 'tf_lst' : [xxx,xxx,...]\n",
    "        ├── 'canonical_name 2'\n",
    "            ├── 'cid' (concept id, or CUI) : xxx\n",
    "            ├── 'definition' : xxx\n",
    "            ├── 'aliases' : [xxx,xxx,...]\n",
    "            ├── 'paper_lst' : [xxx,xxx,...]\n",
    "            ├── 'tf_lst' : [xxx,xxx,...]\n",
    "        ├── 'canonical_name 3'\n",
    "            ├── 'cid' (concept id, or CUI) : xxx\n",
    "            ├── 'definition' : xxx\n",
    "            ├── 'aliases' : [xxx,xxx,...]\n",
    "            ├── 'paper_lst' : [xxx,xxx,...]\n",
    "            ├── 'tf_lst' : [xxx,xxx,...]\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "```\n",
    "In this section, I only processed the paper title, abstract and first 500 words and last 500 words of the body text, because these two parts of the body text can introduce and conclude this paper, which are more representative of the body text. (I tried to process the whole body_text previously, but my computer always froze and could not process all 10000 samples' whole body text, so I sub-sampled the body text to these two parts to represent the whole body text). The process pipeline consists of NER, abbrev resolution, and a further text normalisation using UMLS's concept id (cui). The detailed processing steps are as follows: <br>\n",
    "\n",
    "I utilized the library sciSpaCy 'en_core_sci_sm' pretrained model to perform Named Entity Recognition, and added extra pipeline ,entity_linker with abbreviation resolution, to link the recognized entity to the UMLS knowledge base, where the 'UMLS' stands for the Unified Medical Language System. The entity linker will link the entity to an external medical knowledge base. In this case, I used UMLS, which contains around 3M biomedical concepts. There are other options of the external knowledge base, such as MESH (Medical Subject Headings, 30K entities), but I am using the UMLS, because it is the largest medical knowledge base in SciSpaCy. In this step, the entity linker will map the recognised entity to the known entity (or concept) in th UMLS. This process will also resolve the abbreviations, by adding \"resolve_abbreviations\": True. Abbreviation resolution is an important step because abbreviations are commonly exist in biomedical texts. For each recognised entity, I will only link it to the top 1 match (by setting 'k':1), with a threshold of 0.8 (the default thresh is 0.7, but I tuned it up to 0.8, for a more fine-grained output).\n",
    "Once I match it to a concept entity in UMLS, then I keep its canonical name and other alias recorded in UMLS. If the entities are recognised but not the same as the UMLS canonical name nor any UMLS alias, then this entity name will be added to its alias list as well, and its UMLS canonical name will be the key of this entity entry in the KB. Therefore, during the construction of the KB, the number of entities is growing as the model scanning through the articles, and at the same time, each entities' aliases list are growing too. Furthermore, if the entity is already exist in the KB, its 'paper_lst' is also updated when scanning new articles."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Indexing method"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def generate_inverted_index(KB,processed_df):\n",
    "    inverted_dict = {}\n",
    "    # for key in ['authors']:\n",
    "    for key in ['authors','entities']:\n",
    "        canonical_lst = list(set(KB[key].keys()))\n",
    "        for can in canonical_lst:\n",
    "            inverted_dict.update({can:[]})\n",
    "            if key == 'entities':\n",
    "                for id_tf_tupe in KB['entities'][can]['tf_lst']:\n",
    "                    inverted_dict[can].append(id_tf_tupe)\n",
    "                    inverted_dict[can].append((paper_id,tf))\n",
    "            else:\n",
    "                for paper_id in KB[key][can]['paper_lst']:\n",
    "                    tf = 1 # author name will only apperar once in the paper\n",
    "                    inverted_dict[can].append((paper_id,tf))\n",
    "            inverted_dict[can].sort(key=lambda x: x[0])\n",
    "    return inverted_dict\n",
    "\n",
    "inverted_index = generate_inverted_index(KB,processed_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 paper mentioned COVID-19 (or its aliases):  [0, 1, 5, 6, 7, 8, 9, 10, 11, 12] \n",
      "\n",
      "Show 5 article mentioned mRNA Expression (idx):  [1, 4, 10, 11, 13] \n",
      "\n",
      "Show 5 article mentioned mRNA Expression (title):\n",
      "Similarities and Differences in the Acute-Phase Response to SARS-CoV-2 in Rhesus Macaques and African Green Monkeys\n",
      "Roles of Podoplanin in Malignant Progression of Tumor\n",
      "COVID-19: Clinical features, case fatality, and the effect of symptoms on mortality in hospitalized cases in Iran\n",
      "Coronary Thrombo-Embolic Events after Covid-19 Vaccination- A Single Centre Study\n",
      "Do suicide rates in children and adolescents change during school closure in Japan? The acute effect of the first wave of COVID-19 pandemic on child and adolescent mental health\n",
      " \n",
      " len inverted index: 126308\n"
     ]
    }
   ],
   "source": [
    "print('First 10 paper mentioned COVID-19 (or its aliases): ',[lst[0] for lst in inverted_index['COVID-19'][:10]],'\\n')\n",
    "print('Show 5 article mentioned mRNA Expression (idx): ', [lst[0] for lst in inverted_index['mRNA Expression'][:5]],'\\n')\n",
    "print('Show 5 article mentioned mRNA Expression (title):')\n",
    "for id in inverted_index['mRNA Expression'][:5]:\n",
    "    title = processed_df.loc[processed_df['paper_id']==id[0]]['title'].values[0]\n",
    "    print(title)\n",
    "\n",
    "print(f' \\n len inverted index: {len(inverted_index)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This section, the article indexing method is inverted index by named entities.\n",
    "<br>\n",
    " Each canonical name is a key in inverted_index dictionary, its value is a list of tuples, where each tuple contains a id of the paper that mentioned this entity, and the term frequency of this entity in this paper (tf). By utilizing the information in the KB, it's very efficient to build the inverted index. Because these information can be directly obtained from the KB. Note, the term frequency here is the value after the text normalisation, because the term frequency in the KB considered the UMLS alias and other names mapped to the same UMLS concept as well. <br>\n",
    "\n",
    "With the help of inverted index, we can significantly reduce the search space during the retrieval of documents."
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 40914/60760 [09:44<08:44, 37.81it/s]"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067cebcf",
   "metadata": {},
   "source": [
    "### 4. Text matching utility"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### GloVe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This function will read the clove file, and create a dictionary for our vocabulary\n",
    "def build_glove_embedding_dict():\n",
    "    glove_final_dict_path = 'glove_final_dict.pkl'\n",
    "    if os.path.isfile(glove_final_dict_path):\n",
    "        with open(glove_final_dict_path, 'rb') as kb_pickled:\n",
    "            embedding_vector = pickle.load(kb_pickled)\n",
    "    else:\n",
    "        WV_PATH = 'glove.840B.300d.txt'\n",
    "        embedding_vector = {}\n",
    "        f = open(WV_PATH,encoding=\"utf8\") #embedding dim = 300\n",
    "        for line in tqdm.tqdm(f,desc = 'Building word vector dictionary'):\n",
    "            value = line.split(' ')\n",
    "            word = value[0]\n",
    "            coef = np.array(value[1:],dtype = 'float32')\n",
    "            embedding_vector[word] = coef\n",
    "        f.close()\n",
    "        print('Loaded %s word vectors.' % len(embedding_vector))\n",
    "        with open(glove_final_dict_path,'wb') as f_pickled:\n",
    "            pickle.dump(embedding_vector,f_pickled)\n",
    "            print('glove_final_dict saved successfully!')\n",
    "    return embedding_vector\n",
    "\n",
    "glove_dict = build_glove_embedding_dict()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I used the pre-trained embedding layer GloVe 840B 300d.(download through: https://nlp.stanford.edu/projects/glove/) GloVe is pre-trained to capture the context of the word in the embedding through explicitly capturing the co-occurrence probability. It aims to be built not just on the word probabilities, but their co-occurrence probabilities within the context, and this is used in its loss function in its pre-train process. <br>\n",
    "\n",
    "Steps: <br>\n",
    "Download Glove, open the text file, and create a dictionary with word as a key and its corresponding embedding vector as its value. <br>\n",
    "\n",
    "Note: I found GloVe is not very good when working with biomedical text, because GloVe is not sufficiently trained on biomedical texts. For example, the Glove I am using (840B 300d) was just trained on common crawled text. There are many medical word not in Glove Embedding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Check the percentage of words mentioned in KB and also exists in GloVe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def check_glove_available_percentage(glove_dict,KB):\n",
    "    all_ent = []\n",
    "    for can in list(set(KB['entities'].keys())):\n",
    "        all_ent.append(can)\n",
    "        alias_lst = list(KB['entities'][can]['aliases'])\n",
    "        all_ent.extend(alias_lst)\n",
    "    unique_token = list({token for ent in all_ent for token in ent.split(' ')})\n",
    "    num_token_exist = 0\n",
    "    for token in unique_token:\n",
    "        if token in glove_dict.keys():\n",
    "            num_token_exist += 1\n",
    "    percent = num_token_exist/len(unique_token)\n",
    "    print(f\"There are {len(unique_token)} tokens exist in the KB, and {num_token_exist} of these tokens are available in GloVe ({round(percent*100,2)}% vocabulary availabel in GloVe)\")\n",
    "    return\n",
    "check_glove_available_percentage(glove_dict,KB)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The lack of medical vocabulary is the major drawback of using GloVe embedding in this assignment. However, using pretrained word embedding is asked in the assignment step 4(d). Therefore, I am still going to implement my IR system using GloVe. Furthermore, integrating the pre-trained word embedding into the Information Retrival System can generally improve the performance of the retrival, because TF-IDF cannot imply the semantic similarity, whereas, the pre-trained word embeddings learned the semantic meaning of the words over a large amount of text. Therefore, I am going to integrate GloVe into the IR System.\n",
    "<br>\n",
    "Potential Improvement: <br>\n",
    "1. It's better to use a word embedding that can capture more medical terms' meanings, instead of using general-purpose word embedding GloVe. I did some research on this and found a publically available biomedical word embedding 'cui2vec' (GitHub: https://github.com/beamandrew/cui2vec , download: https://figshare.com/s/00d69861786cd0156d81 , paper: https://arxiv.org/abs/1804.01486 ). 'cui2vec' can map UMLS concept unique identifier (cui) to a vector. This implies 'cui2vec' word embedding is highly aligned and consistent with our knowledge base that is built with UMLS concepts as well. However, 'cui2vec' does not cover a lot of the entity concepts that are in out CORD-19 article KB (only 32.43% available, which is even worse than GloVe. 'cui2vec' is in entity phrase levels, GloVe is in word token level, but generally speaking, 32.43% in entity level is worse than 44.63% in token level, because even there is one unknown word in a medical phrase, but if we can embed other tokens, the model can still contain partial information of this token). Other than 'cui2vec', there is an available 'BioWordVec'. 'BioWordVec' is a biomedical word embedding pre-trained on PubMed+MIMIC-III using FastText, but this embedding is too big (13GB) for my local computer. Therefore, I am going to use GloVe embedding, which can be used on my local PC, and also covers larger amount of medical terms compared with 'cui2vec'. However, if I have a computer with better computation power and bigger memory, I will definitely give 'BioWordVec' a try.\n",
    "<br>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### The code below shows the concept vocabulary of cui2vec word embedding, which indicates cui2vec covers less than GloVe for our KB."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_med_embedding_dict():\n",
    "    med_final_dict_path = 'med_final_dict.pkl'\n",
    "    if os.path.isfile(med_final_dict_path):\n",
    "        with open(med_final_dict_path, 'rb') as kb_pickled:\n",
    "            embedding_vector = pickle.load(kb_pickled)\n",
    "    else:\n",
    "        WV_PATH = 'cui2vec_pretrained.csv'\n",
    "        embedding_vector = {}\n",
    "        df = pd.read_csv(WV_PATH)\n",
    "        dim_cols = ['V'+str(i) for i in range(1,501)]\n",
    "        df.columns = ['cui']+dim_cols\n",
    "        df['vec'] = df[dim_cols].values.tolist()\n",
    "        df.drop(columns=dim_cols, inplace=True)\n",
    "        for cid,wv in zip(df['cui'],df['vec']):\n",
    "            embedding_vector.update({cid:np.array(wv)})\n",
    "\n",
    "        print('Loaded %s word vectors.' % len(embedding_vector))\n",
    "        with open(med_final_dict_path,'wb') as f_pickled:\n",
    "            pickle.dump(embedding_vector,f_pickled)\n",
    "            print('med_final_dict saved successfully!')\n",
    "\n",
    "    return embedding_vector\n",
    "\n",
    "med_dict = build_med_embedding_dict()\n",
    "print(f'The number of concept unique identifier in cui2vec is: {len(med_dict)}')\n",
    "\n",
    "\n",
    "def check_med_available_percentage(med_dict,KB):\n",
    "    all_cui_kb = [ent_dict['cid'].strip() for ent_dict in KB['entities'].values() if isinstance(ent_dict,dict)]\n",
    "    num_ent_exist = 0\n",
    "    for concept in all_cui_kb:\n",
    "        if concept in med_dict.keys():\n",
    "            num_ent_exist += 1\n",
    "    percent = num_ent_exist/len(all_cui_kb)\n",
    "    print(f\"There are {len(all_cui_kb)} entity cui exist in the KB, and {num_ent_exist} concpets in KB are available in cui2vec embedding ({round(percent*100,2)}% entities availabel in cui2vec)\")\n",
    "    return\n",
    "check_med_available_percentage(med_dict,KB)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Due to the lack of coverage of medical terminologies in GloVe embedding, I am going to use both TF-IDF and pre-trained GloVe word embeddings in my IR system. By combining these two scores, the retrival performance can be more robust, especially in the cases of out-of-vocabulary in GloVe. The detailed implementation of my IR system as follows:\n",
    "\n",
    "- query pre-processing:\n",
    "    - In my IR system, I pre-process the query by removing the stopwords. Then perform NER by using two models, one is 'en_core_sci_sm' for medical entity recognition, and 'en_core_web_sm' for general named entity recognition. This is because the SciSpaCy medical NER model are not good at recognising general entities, such as, author names, etc. Therefore, we also need a general NER model too.\n",
    "- matching common entities and get the posting list to reduce the paper search space:\n",
    "    - if the entity in the query is matched to a canonical name in the inverted index key, then we store this entity for further embedding and tf-idf computation, also get id of the articles mentioned this entity.\n",
    "    - if this entity in the query is an alias (other associated names) of a canonical name, then we will get its canonical name. Then do the same steps above.\n",
    "    - If query entity is neither a canonical name nor an alias, then I compute the cosine similarity of this query entity with all the canonical names in the inverted index. if all the similarities are < 0.7, it won't match to any doc canonical entities, and we will ignore this query entity. Otherwise, the query entity will match to the canonical name with the highest similarity score. (This text normalisation step can also be done with the scispacy UMLS entitylinker, but it's a very heavy model, and it is very time-consuming. Therefore, I am utilising the cosine similarity of the embedding to perform the soft match. Normalising the query text is important, because there will be query entities that are neither the canonical name nor the known aliases in the KB, but they might describe the same concept. Using cosine similarity to perform the soft match can mitigate this issue)\n",
    "    - Finally, the query will be represented by these matched entities, and at the same time we also obtained a reduced doc search space from inverted index method\n",
    "- Compute the score for all the doc in the reduced search space\n",
    "    - The score of the retrival rank contains two parts, the tf-idf and the cosine similarity of the query and document vectors\n",
    "        - The query and the document are both represented by their recognised entities. If an entity is composed of multiple word tokens, then this entity will be represented as a mean-pooling of all the tokens, therefore, all the entity embeddings are 300d (the dimension is the same as the GloVe embedding for a single token). Then the query embedding and the document embedding vector is the mean-pooling vector of all their recognised entities, so the query and the documents vectors are both 300d.\n",
    "        - To alleviate the limited medical vocabulary coverage of the GloVe embedding, I utilised both tf-idf and the GloVe embedding cosine similarity.\n",
    "        - I first standardise the embedding cosine similarity list and the tfidf list both in the range of 0-1. For each document I averaged its standardised tf-idf score and the standardised embedding cosine score, and this average will be the final score of ranking. By doing this, some rare medical words that are not in the GloVe embedding can still obtain some ranking score from tf-idf part. There are some out-of-vocabulary retrival test examples below.\n",
    "        - Finally, rank the documents based on the scores.\n",
    "\n",
    "##### The detailed implementation is shown in the code below:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Retriever_glove:\n",
    "    def __init__(self,inverted_index,glove_dict,KB,data_df):\n",
    "        self.nlp = spacy.load(\"en_core_sci_sm\") # for medical NER on query\n",
    "        self.nlp_general = spacy.load(\"en_core_web_sm\") # for general NER on query, such as author names, etc\n",
    "        self.word_embedding = glove_dict\n",
    "        self.inverted_index = inverted_index\n",
    "        self.KB = KB\n",
    "        self.norm_dict = self.__canonical_aliases_normalization()\n",
    "        self.data_df = data_df\n",
    "        return\n",
    "\n",
    "    def __get_vec(self,word):\n",
    "        if word in self.word_embedding.keys():\n",
    "            return self.word_embedding.get(word)\n",
    "        else:\n",
    "            # if the word is unknown in GloVe, then return a zero vector with the same dim i.e. 300\n",
    "            return np.zeros(300)\n",
    "\n",
    "    def __embed_text(self,ent_lst):\n",
    "        entity_embedding_dict = {}\n",
    "        for ent in ent_lst:\n",
    "            # if the entity is a term consists of multiple word tokens, then just avergae it\n",
    "            if ' ' in ent:\n",
    "                stack = np.vstack((self.__get_vec(w) for w in ent.split()))\n",
    "                ent_vector = np.mean(stack,axis = 0)\n",
    "            else:\n",
    "                ent_vector = self.__get_vec(ent)\n",
    "            entity_embedding_dict.update({ent:ent_vector})\n",
    "        return entity_embedding_dict\n",
    "\n",
    "    def __canonical_aliases_normalization(self):\n",
    "        norm_dict = {}\n",
    "        for can in list(set(self.KB['entities'].keys())):\n",
    "            for alias in list(self.KB['entities'][can]['aliases']):\n",
    "                norm_dict.update({alias:can})\n",
    "        return norm_dict\n",
    "\n",
    "    def __cal_wv_similarity(self,entity_vec_dict1,entity_vec_dict2):\n",
    "        # if the text contain multiple entities,\n",
    "        # then the vector that represents this text is the average of all the entity vector\n",
    "        for i, embedding_dict in enumerate([entity_vec_dict1,entity_vec_dict2]):\n",
    "            # if there's no embedding, then return similarity of 0\n",
    "            if len(embedding_dict) == 0:\n",
    "                return 0\n",
    "            if len(embedding_dict) == 1:\n",
    "                text_vec = list(embedding_dict.values())[0]\n",
    "            else:\n",
    "                stack = np.vstack((v for v in embedding_dict.values()))\n",
    "                text_vec = np.mean(stack,axis = 0)\n",
    "            if i == 0:\n",
    "                text_vec1 = text_vec\n",
    "            else:\n",
    "                text_vec2 = text_vec\n",
    "\n",
    "        if norm(text_vec1) == 0 or norm(text_vec2) == 0:\n",
    "            return 0\n",
    "        cos_sim = dot(text_vec1, text_vec2)/(norm(text_vec1)*norm(text_vec2))\n",
    "        return cos_sim\n",
    "\n",
    "    def __cal_tfidf_similarity(self,matched_ent,paper_id):\n",
    "        tfidf_score = 0\n",
    "        for ent in matched_ent:\n",
    "            if len([ele[1] for ele in self.inverted_index.get(ent) if ele[0] == paper_id]) != 0:\n",
    "                t_count = [ele[1] for ele in self.inverted_index.get(ent) if ele[0] == paper_id][0]\n",
    "                tf_w = 1 + math.log10(t_count)\n",
    "                N = 10000\n",
    "                df = len(self.KB['entities'][ent]['paper_lst'])\n",
    "                idf_w = math.log10((N/df))\n",
    "                tfidf_score += tf_w*idf_w\n",
    "        # doc_tfidf_norm = self.__get_doc_tfidf_norm(paper_id)\n",
    "        # tfidf_score = tfidf_score/doc_tfidf_norm\n",
    "        return tfidf_score\n",
    "\n",
    "    def __show_result(self,paper_selected):\n",
    "        for rank in range(1,len(paper_selected)+1):\n",
    "            idx = rank -1\n",
    "            snippet = self.data_df.loc[self.data_df['paper_id']==paper_selected[idx][0]]['original_abstract'].values[0]\n",
    "            title = self.data_df.loc[self.data_df['paper_id']==paper_selected[idx][0]]['title'].values[0]\n",
    "            authors = self.data_df.loc[self.data_df['paper_id']==paper_selected[idx][0]]['authors'].values[0]\n",
    "            article_identifier = self.data_df.loc[self.data_df['paper_id']==paper_selected[idx][0]]['paper_identifier'].values[0]\n",
    "            article_number = self.data_df.loc[self.data_df['paper_id']==paper_selected[idx][0]]['paper_id'].values[0]\n",
    "            score = paper_selected[idx][1]\n",
    "            if rank != 1:\n",
    "                print('\\n')\n",
    "            print(f'------------------------------ Match Ranked {rank} (similarity score: {score})------------------------------')\n",
    "            print(f'article identifier: {article_identifier},  article number: {article_number} \\n')\n",
    "            print(f'title:  {title} \\n')\n",
    "            print('Authors: ')\n",
    "            print('; '.join(authors),'\\n')\n",
    "            print('snippet: \\n')\n",
    "            print(snippet)\n",
    "\n",
    "    def retrive(self,query,show = True):\n",
    "        stops = stopwords.words('english') # english stopwords, because our data is in english\n",
    "        query = \" \".join([word for word in query.split() if word not in stops])\n",
    "        query = query.replace('[^A-Za-z0-9\\s+\\-]',' ')\n",
    "        q_doc = self.nlp(query)\n",
    "        q_doc_general = self.nlp_general(query)\n",
    "        q_entities = list(set([ent.text.strip() for ent in  q_doc.ents] + [ent.text.strip() for ent in  q_doc_general.ents]))\n",
    "        query_ent_vec_dict = self.__embed_text(q_entities)\n",
    "\n",
    "        doc_lst = []\n",
    "        matched_ent = []\n",
    "        for ent in q_entities:\n",
    "            # try hard match first\n",
    "            if ent in self.inverted_index.keys():\n",
    "                paper_lst = [ele[0] for ele in self.inverted_index.get(ent)]\n",
    "                doc_lst.extend(paper_lst)\n",
    "                matched_ent.append(ent)\n",
    "            elif ent in self.norm_dict.keys():\n",
    "                can = self.norm_dict[ent]\n",
    "                paper_lst = [ele[0] for ele in self.inverted_index.get(can)]\n",
    "                doc_lst.extend(paper_lst)\n",
    "                matched_ent.append(can)\n",
    "            # if can't hard match, use word vector to soft match them with a threshold\n",
    "            else:\n",
    "                ENT_SOFT_MATCH_THRESH = 0.7\n",
    "                can_lst = list(set(self.norm_dict.values()))\n",
    "                sim_lst = []\n",
    "                for can in can_lst:\n",
    "                    can_vec_dict = self.__embed_text([can])\n",
    "                    q_ent_vec_dict = {ent:query_ent_vec_dict[ent]}\n",
    "\n",
    "                    # if any of these two entities are completely out of vocab in GloVe, skip match them\n",
    "                    if np.all(can_vec_dict.get(can)==0) or np.all(q_ent_vec_dict.get(ent)==0):\n",
    "                        continue\n",
    "\n",
    "                    sim_lst.append(self.__cal_wv_similarity(q_ent_vec_dict,can_vec_dict))\n",
    "                if len(sim_lst) == 0:\n",
    "                    continue\n",
    "                max_sim = max(sim_lst)\n",
    "                if max_sim > ENT_SOFT_MATCH_THRESH:\n",
    "                    max_idx = sim_lst.index(max_sim)\n",
    "                    can = can_lst[max_idx]\n",
    "                    paper_lst = [ele[0] for ele in self.inverted_index.get(can)]\n",
    "                    doc_lst.extend(paper_lst)\n",
    "                    matched_ent.append(can)\n",
    "\n",
    "        doc_to_be_searched = list(set(doc_lst))\n",
    "        wv_score = []\n",
    "        tfidf_score = []\n",
    "        for paper_id in doc_to_be_searched:\n",
    "            doc_ent_lst = self.KB['paper'][paper_id]['entities'] + self.KB['paper'][paper_id]['authors']\n",
    "            doc_ent_vec_dict = self.__embed_text(doc_ent_lst)\n",
    "            wv_score.append(self.__cal_wv_similarity(doc_ent_vec_dict,query_ent_vec_dict))\n",
    "            tfidf_score.append(self.__cal_tfidf_similarity(matched_ent,paper_id))\n",
    "\n",
    "        # standardising the wv similarity score and tfidf score to 0-1 before averaging the two scores\n",
    "        if len(tfidf_score) == 0 or max(tfidf_score) == min(tfidf_score):\n",
    "            std_tfidf_score = [0]*len(tfidf_score)\n",
    "        else:\n",
    "            std_tfidf_score = [(s - min(tfidf_score))/(max(tfidf_score)-min(tfidf_score)) for s in tfidf_score]\n",
    "\n",
    "        if len(wv_score) == 0 or max(wv_score) == min(wv_score):\n",
    "            std_wv_score = [0]*len(wv_score)\n",
    "        else:\n",
    "            std_wv_score = [(s - min(wv_score))/(max(wv_score)-min(wv_score)) for s in wv_score]\n",
    "\n",
    "        # The final retrieval score is the average of the standardised tfidf and the standardised wv cos similarity\n",
    "        scores_lst = [(glove+tfidf)/2 for glove,tfidf in zip(std_wv_score,std_tfidf_score)]\n",
    "        scores_arr = np.array(scores_lst)\n",
    "        ranked_idx = np.argsort(scores_arr)[::-1]\n",
    "        paper_ranked = [(doc_to_be_searched[idx],scores_arr[idx]) for idx in ranked_idx]\n",
    "        if len(paper_ranked) == 0:\n",
    "            print('Sorry, there is no good match :(')\n",
    "        else:\n",
    "            if show == True:\n",
    "                self.__show_result(paper_ranked[:3])\n",
    "            else:\n",
    "                return paper_ranked\n",
    "\n",
    "    def eval(self,queries,labels):\n",
    "        # this function return mrr metric\n",
    "        RR_lst = []\n",
    "        for q,l in zip(queries,labels):\n",
    "            paper_ranked = self.retrive(q,show = False)\n",
    "            paper_id_ranked = [tup[0] for tup in paper_ranked]\n",
    "            if l in paper_id_ranked:\n",
    "                RR = 1/(paper_id_ranked.index(l)+1) # '+1' here is because we are using the position not the index\n",
    "            else:\n",
    "                RR = 0\n",
    "            RR_lst.append(RR)\n",
    "        return sum(RR_lst)/len(RR_lst)\n",
    "\n",
    "IR = Retriever_glove(inverted_index,glove_dict,KB,processed_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "How useful NER and KB for this task?\n",
    "- NER: With the help of NER, the number of unique words are significantly reduced and thus improved the time efficiency (without big drop of retrieval performance). Because the conventional IR using inverted index method is utilising all the unique words, which is not a desired method especially when the document collection is huge. After performing NER, the IR system can focus more on the context, so even we reduce the space of inverted index, the IR system can still maintain a good performance. Furthermore, using unique words will break the entity phrases apart, and twisted the representation of the original meaning. Therefore, we need NER to maintain the integrity of the context.\n",
    "- KB: The Knowledge base enable an easy implementation of the retrieval system by collecting all the crucial information of the document collection. In the KB, each document is represented as its entities, so people do not need to go through the entire text. The most important part of KB is the storage of canonical names and the associated aliases for text normalisation. In real life, the ways of conveying the same concept in natural language could be various. And text normalisation could enable the IR system to capture the concepts correctly under these variations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query = 'what are the symptoms of covid19?'\n",
    "IR.retrive(query)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- By reading and analysing the text snippet myself, I found these top 2 articles are highly relevant to the symptoms of COVID, the performance of retrival is really well. For example, the 1st one talks about the possible psychosis symptoms of covid, e.g.'patients with a novel corona virus had psychotic symptoms, including hallucination in different forms of modality, delusion, disorganized speech, and grossly disorganized or catatonic behaviors'. The 2nd article mainly talks about the headache symptoms of covid specifically."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summarization -- Duc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize, word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "class NLTK_Extractive_Summarizer:\n",
    "    \"\"\"\n",
    "    A class used to summarize texts.\n",
    "    Attributes\n",
    "    ----------\n",
    "    stop_words : set\n",
    "        a set of stopwords. These are ignored when searching for the most used\n",
    "        words in the text\n",
    "    language : str\n",
    "        the current selected language. The stop words set is language specific\n",
    "    summary_length : int\n",
    "        the default number of sentences to use for the summary.\n",
    "    balance_length : bool\n",
    "        determines if the sentence weight is weighted based on sentence length\n",
    "    \"\"\"\n",
    "\n",
    "    stop_words = {}\n",
    "    language = None\n",
    "    summary_length = 0\n",
    "    balance_length = False\n",
    "\n",
    "    def __init__(self, language='english', summary_length=3, balance_length=True):\n",
    "        \"\"\"\n",
    "        :param str language: The language to use, defaults to 'en'\n",
    "        :param int summary_length: The default number of sentences in summary, defaults to 3\n",
    "        :param bool balance_length: Balance sentences on length, default to False\n",
    "        \"\"\"\n",
    "\n",
    "        # Set the language to use and set the stop words to the default\n",
    "        # list provided by NLTK corpus for this language\n",
    "        self.stop_words = set(stopwords.words(language))\n",
    "        self.language = language\n",
    "\n",
    "        # Set the default length for the summaries to be created\n",
    "        self.summary_length = summary_length\n",
    "\n",
    "        # Sets the switch if the sentence weights need to weighted on\n",
    "        # sentence length. This might improve performance if the text\n",
    "        # contains a variety of short and long sentences.\n",
    "        self.balance_length = balance_length\n",
    "\n",
    "    def weighted_sentence(self, text, summary_length=None):\n",
    "        \"\"\"\n",
    "        Summarize the given text based on the weight of sentence\n",
    "        The language and stop word set have been initialized and are used. If no\n",
    "        summary length is given as parameter, the default length is used.\n",
    "        :param (str or list) text: The text to summarize\n",
    "        :param int summary_length: The number of sentences in summary, optional\n",
    "        :return (str): A string with the summary of the given text\n",
    "        \"\"\"\n",
    "\n",
    "        # Length of summary to generate, if not specified use default\n",
    "        if not summary_length:\n",
    "            summary_length = self.summary_length\n",
    "\n",
    "        # Make a list of all the sentences in the given text\n",
    "        sentences = []\n",
    "        if type(text) == str:\n",
    "            sentences.extend(tokenize.sent_tokenize(text))\n",
    "        elif type(text) == list:\n",
    "            for text_part in text:\n",
    "                sentences.extend(tokenize.sent_tokenize(text_part))\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "        # Determine for each word, not being a stop word, the number of occurences\n",
    "        # in the text. This word frequency determines the importance of the word.\n",
    "        word_weights={}\n",
    "        for sent in sentences:\n",
    "            for word in word_tokenize(sent):\n",
    "                word = word.lower()\n",
    "                if len(word) > 1 and word not in self.stop_words:\n",
    "                    if word in word_weights.keys():\n",
    "                        word_weights[word] += 1\n",
    "                    else:\n",
    "                        word_weights[word] = 1\n",
    "\n",
    "        # The weight of each sentence equals the sum of the word importance for\n",
    "        # each word in the sentence\n",
    "        sentence_weights = {}\n",
    "        for sent in sentences:\n",
    "            sentence_weights[sent] = 0\n",
    "            tokens = word_tokenize(sent)\n",
    "            for word in tokens:\n",
    "                word = word.lower()\n",
    "                if word in word_weights.keys():\n",
    "                    sentence_weights[sent] += word_weights[word]\n",
    "            if self.balance_length and (len(tokens) > 0):\n",
    "                sentence_weights[sent] = sentence_weights[sent] / len(tokens)\n",
    "        highest_weights = sorted(sentence_weights.values())[-summary_length:]\n",
    "\n",
    "\n",
    "        # The summary consists of the sentences with the highest sentence weight, in the\n",
    "        # same order as they occur in the original text\n",
    "        summary = \"\"\n",
    "        for sentence, strength in sentence_weights.items():\n",
    "            if strength in highest_weights:\n",
    "                summary += sentence + \" \"\n",
    "        summary = summary.replace('_', ' ').strip()\n",
    "\n",
    "        return summary\n",
    "\n",
    "\n",
    "    def text_rank(self, text, summary_length=None):\n",
    "        \"\"\"\n",
    "        Summarize the given text using text rank algorithm (based on page rank)\n",
    "        The language and stop word set have been initialized and are used. If no\n",
    "        summary length is given as parameter, the default length is used.\n",
    "        :param (str or list) text: The text to summarize\n",
    "        :param int summary_length: The number of sentences in summary, optional\n",
    "        :return (str): A string with the summary of the given text\n",
    "        \"\"\"\n",
    "\n",
    "        # Length of summary to generate, if not specified use default\n",
    "        if not summary_length:\n",
    "            summary_length = self.summary_length\n",
    "\n",
    "        # Make a list of all the sentences in the given text\n",
    "        sentences = []\n",
    "        if type(text) == str:\n",
    "            sentences.extend(tokenize.sent_tokenize(text))\n",
    "        elif type(text) == list:\n",
    "            for text_part in text:\n",
    "                sentences.extend(tokenize.sent_tokenize(text_part))\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "        # remove punctuations, numbers and special characters\n",
    "        clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "        # make alphabets lowercase\n",
    "        clean_sentences = [s.lower() for s in clean_sentences]\n",
    "\n",
    "        # function to remove stopwords\n",
    "        def remove_stopwords(sen):\n",
    "            sen_new = \" \".join([i for i in sen if i not in self.stop_words])\n",
    "            return sen_new\n",
    "\n",
    "\n",
    "        # remove stopwords from the sentences\n",
    "        clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "\n",
    "        # Extract word vectors\n",
    "        word_embeddings = {}\n",
    "        with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                word_embeddings[word] = coefs\n",
    "\n",
    "\n",
    "        # Init sentence vectors\n",
    "        sentence_vectors = []\n",
    "        for sent in clean_sentences:\n",
    "            if len(sent) != 0:\n",
    "                vec = sum([word_embeddings.get(word, np.zeros((100,))) for word in sent.split()]) / (len(sent.split()) + 0.001)\n",
    "            else:\n",
    "                vec = np.zeros((100,))\n",
    "            sentence_vectors.append(vec)\n",
    "\n",
    "        # similarity matrix\n",
    "        sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "\n",
    "        for i in range(len(sentences)):\n",
    "            for j in range(len(sentences)):\n",
    "                if i != j:\n",
    "                    sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
    "                                                      sentence_vectors[j].reshape(1,100))[0, 0]\n",
    "\n",
    "\n",
    "        # Compute score\n",
    "        nx_graph = nx.from_numpy_array(sim_mat)\n",
    "        scores = nx.pagerank(nx_graph)\n",
    "        ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "\n",
    "        summ = []\n",
    "        # Generate summary\n",
    "        for i in range(summary_length):\n",
    "            summ.append(ranked_sentences[i][1])\n",
    "        return \" \".join(summ)\n",
    "\n",
    "\n",
    "    def summarize(self, long_text, summary_length=None, split_at=50, method=1):\n",
    "        \"\"\"\n",
    "        Summarize the long text using two method: weighted sentence and text rank.\n",
    "        The language and stop word set have been initialized and are used. If no\n",
    "        summary length is given as parameter, the default length is used.\n",
    "\n",
    "        :param (str or list) long_text: The long text to summarize\n",
    "        :param int summary_length: The number of sentences in summary, optional\n",
    "        :param int split_at: The number of sentences in each text chunk\n",
    "        :param int method: 1: weighted sentence, 2: text rank\n",
    "\n",
    "        :return (str): A string with the summary of the given text\n",
    "        \"\"\"\n",
    "\n",
    "        # Length of summary to generate, if not specified use default\n",
    "        if not summary_length:\n",
    "            summary_length = self.summary_length\n",
    "\n",
    "        # Make a list of all the sentences in the given text and split this list\n",
    "        # in chunks of n sentences, n being the split_at value\n",
    "        sentences = []\n",
    "        for sent in tokenize.sent_tokenize(long_text):\n",
    "            sentences.append(sent)\n",
    "\n",
    "        chunks = [sentences[x:x+split_at] for x in range(0, len(sentences), split_at)]\n",
    "\n",
    "        # Choose method applied to summarize\n",
    "        method_func = self.weighted_sentence if method == 1 else self.text_rank\n",
    "        summaries = []\n",
    "        for sentences in chunks:\n",
    "            summary = method_func(sentences, summary_length)\n",
    "            summaries.append(summary)\n",
    "\n",
    "        return \" \".join(summaries)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: : 546\n",
      "33442\n",
      "In this section, we provide an overview of South Africa’s demographics as a framework to assess the curriculum recovery plan. Next, we describe the context to which the South African curriculum recovery plan applies. To counteract these difficulties, quintile 1 schools receive more per-capita funding from the state than quintile 5 schools. Wealthy schools have continued to provide learning opportunities for students by preparing take-home learning materials for them, for home-schooling. It is clear that excising aspects of the curriculum is at the discretion of schools. Clearly, this complex relationship—of teachers’ insufficient content knowledge, inadequate curriculum coverage, and unfavorable learner performance—persists. Curriculum reorganisation involves reorganisation and refocusing of the curriculum to make the content more manageable. The aim of curriculum reorganization is to manage content by repackaging and integrating subjects to reduce content overload. This complex content would require highly skilled teachers with deep insights in and aptitudes for curriculum design, selection, and hybridization. To disrupt the cycle of impoverished teaching and learning, teachers would have to possess advanced pedagogical and content knowledge to ensure that complex content is taught and learned effectively. As public intellectuals, curriculum decision-makers should be mindful of the contexts and constraints surrounding curriculum making. The curriculum recovery plan is explicit about what needs to be recouped: time and curriculum coverage. A crisis in education requiring urgent curriculum reform is not a new phenomenon. Two options are available to curriculum designers: curriculum change and curriculum continuity (Jansen 1990). It is by portraying the curriculum as an expression of the political interests, knowledge, and values of a dominant force that one has persuasive insights into curriculum predicaments and into new developments in curriculum reform (Jansen 1990). On the bright side, Covid-19 has forced curriculum makers to critically reevaluate each subject in the school curriculum, though they may delay action until after the crisis subsides. On the upside, the revision of the DBE policy forced curriculum makers to evaluate the curriculum critically. The policy’s revisions—those of curriculum trimming and curriculum reorganization—give an idea of how to begin to negate inequalities. Because most teachers do not have curriculum decision-making experience or capacity, the role of a school principal as curriculum leader becomes crucial. Successful curriculum revision will also require changes in the criteria for appointing school leaders. In his words, “Knowledge of the powerful is defined by who gets the knowledge in a society” (2008, p. 14), while powerful knowledge “refers not to the backgrounds of those who have most access to knowledge or who give it legitimacy” but “to what the knowledge can do or what intellectual power it gives to those who have access to it” (Young 2008, p. 14). In other words, individuals from poor socioeconomic backgrounds need not lead a life of permanent existence on the margins—curriculum revisioning that is planned around the notion of “powerful knowledge” can reduce knowledge and socioeconomic inequalities. Accessing powerful knowledge is a fundamental right for all students and not a privilege for the children of the elite; the knowledge that is not available at home is the knowledge that should be acquired at school (Young 2008). Accessing powerful knowledge is about giving all learners from poorer schools, through curriculum revision, access to trustworthy knowledge of the world and opportunities to succeed in school and thereafter.\n"
     ]
    }
   ],
   "source": [
    "extractor = NLTK_Extractive_Summarizer(summary_length = 5)\n",
    "id = 0\n",
    "text = processed_df.loc[processed_df['paper_id']==id]['original_body_text'].values[0]\n",
    "# print(text)\n",
    "summ = extractor.summarize(text)\n",
    "print('len: :', len(summ.split()))\n",
    "print(len(text))\n",
    "print(summ)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SciBERT Summarizer -- Sinuo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: allenai/scibert_scivocab_uncased\n",
      "Response Time: 5.9837806224823\n",
      "Summary:  In this section, we provide an overview of South Africa’s demographics as a framework to assess the curriculum recovery plan. Unsurprisingly, impoverished quintile 1, 2 and 3 schools provide inadequate preparation of students for a poverty-free future and, more troubling, represent a bimodal school system:South Africa is still a tale of two schools: One which is functional, wealthy, and able to educate students; with the other being poor, dysfunctional, and unable to equip students with the necessary numeracy and literacy skills they should be acquiring in primary school. ( I stopped schooling in standard 5, back in the day, because there was not enough money to take me and my siblings to school. The perpetuation of poor education outcomes in South Africa is a major impediment to individual, social, and economic advancement. The analysis of the two excerpts also reveals that the DBE is aware of the deficiencies of the present curriculum, which include irrelevant and outdated content. Of equal concern is the awareness that teachers “do not know how to transform the system” (DBE 2020a, p. 15). The nation’s large cohort of unqualified or underqualified teachers adversely influences the quality of curriculum coverage and the delivery of content, and poses a serious problem for managing curriculum revision (Maphalala et al. To disrupt the cycle of impoverished teaching and learning, teachers would have to possess advanced pedagogical and content knowledge to ensure that complex content is taught and learned effectively. So, what shape could visionary leadership take? In explaining the recommendations, the curriculum recovery plan makes visible some of the factors that work against the efficient and effective provision of education, which would require undertaking valiant measures like reducing teacher absenteeism and removing ineffective school leaders—two important factors identified by Spaull (2012) as correlated to a bimodal education system. As such, decision makers must understand the subtleties of political thinking, the historical government systems, the varied constructions of power, and those in positions of authority (Ball 2007). Specifically, this translates into using power positions to improve the lives of students and the working conditions of teachers. 2007) within and outside the policy itself. In the case at hand, the curriculum recovery plan is just that: an interim set of measures to deal with the aftereffects of the closure of schools for more than two months and the resultant loss of contact time. What is the value in recovering a curriculum but then to omit content that can prevent contracting or spreading the disease? Perhaps we need to heed Dewey’s call for education to relate to students’ current lives and not only to prepare them for the future (Paraskeva 2011). Curriculum, as a political expression, is based on the idea that knowledge cannot be impartial. Undoubtedly, teaching, learning, and assessment are at the center of curriculum changes during crisis times, too. They further assume that changes in the curriculum and pedagogy are moderately “evolutionary”, not “revolutionary”, i.e., they are not a radical departure from existing policies (2016, p. 69)—a claim made by Tyack and Cuban (1995) as well. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !pip install bert-extractive-summarizer==0.4.2\n",
    "from summarizer import Summarizer\n",
    "\n",
    "import torch\n",
    "import time\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel\n",
    "# assert torch.cuda.is_available()\n",
    "\n",
    "\n",
    "SUMMARY_RATIO = 0.1\n",
    "body_text = processed_df.loc[processed_df['paper_id']==id]['original_body_text'].values[0]\n",
    "\n",
    "BERTS = ['allenai/scibert_scivocab_uncased']\n",
    "# Evaluate custom models.\n",
    "for BERT_PATH in BERTS:\n",
    "\n",
    "  # Load model, model config and tokenizer via Transformers\n",
    "  custom_config = AutoConfig.from_pretrained(BERT_PATH)\n",
    "  custom_config.output_hidden_states=True\n",
    "  print(custom_config)\n",
    "  custom_tokenizer = AutoTokenizer.from_pretrained(BERT_PATH)\n",
    "  custom_model = AutoModel.from_pretrained(BERT_PATH, config=custom_config)\n",
    "  model = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)\n",
    "\n",
    "\n",
    "  start = time.time()\n",
    "  resp = model(body_text, ratio=SUMMARY_RATIO)\n",
    "  end = time.time()\n",
    "  print('Model type:', BERT_PATH)\n",
    "  print(f'Response Time: {end-start}')\n",
    "  # TODO: Split into sentences and pretty-print.\n",
    "  print('Summary: ', resp, \"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "'In this section, we provide an overview of South Africa’s demographics as a framework to assess the curriculum recovery plan. The curriculum recovery plan is explicit about what needs to be recouped: time and curriculum coverage. On the bright side, Covid-19 has forced curriculum makers to critically reevaluate each subject in the school curriculum, though they may delay action until after the crisis subsides.'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body_text = ' '.join(summ.split()[:512])\n",
    "resp = model(body_text, ratio=SUMMARY_RATIO)\n",
    "resp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ml4pubmed/scibert-scivocab-cased_pub_section\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ml4pubmed/scibert-scivocab-cased_pub_section\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from summarizer import Summarizer\n",
    "\n",
    "body = text\n",
    "model = Summarizer(custom_model=model, custom_tokenizer=tokenizer)\n",
    "model(body)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "jointbert",
   "language": "python",
   "display_name": "jointbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
